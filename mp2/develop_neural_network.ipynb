{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWMWW8Ab_345"
      },
      "source": [
        "# (Optional) Colab Setup\n",
        "If you aren't using Colab, you can delete the following code cell. This is just to help students with mounting to Google Drive to access the other .py files and downloading the data, which is a little trickier on Colab than on your local machine using Jupyter. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vH4wc4iD_6w_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01babae2-47c9-409d-9109-3785c8185104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# you will be prompted with a window asking to grant permissions\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\",force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XpNsPHZc_879",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cb45b7-5ce7-4b4a-b53b-54d7cd7276bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS444_Neha/assignment2\n"
          ]
        }
      ],
      "source": [
        "# fill in the path in your Google Drive in the string below. Note: do not escape slashes or spaces\n",
        "import os\n",
        "datadir = \"/content/assignment2\"\n",
        "if not os.path.exists(datadir):\n",
        "  !ln -s \"/content/drive/MyDrive/CS444_Neha/assignment2/\" $datadir\n",
        "os.chdir(datadir)\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cHqo6b1_Bzk"
      },
      "source": [
        "# Implement a Neural Network\n",
        "\n",
        "This notebook contains testing code to help you develop a neural network by implementing the forward pass and backpropagation algorithm in the `models/neural_net.py` file. \n",
        "\n",
        "You will implement your network in the class `NeuralNetwork` inside the file `models/neural_net.py` to represent instances of the network. The network parameters are stored in the instance variable `self.params` where keys are string parameter names and values are numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nTt_CiWh_Bzm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from models.neural_net import NeuralNetwork\n",
        "\n",
        "# For auto-reloading external modules\n",
        "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\"Returns relative error\"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://www.adeveloperdiary.com/data-science/machine-learning/understand-and-implement-the-backpropagation-algorithm-from-scratch-in-python/ "
      ],
      "metadata": {
        "id": "O_S0ijqrcsb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Neural network model.\"\"\"\n",
        "\n",
        "from collections import defaultdict\n",
        "from typing import Sequence\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"A multi-layer fully-connected neural network. The net has an input\n",
        "    dimension of N, a hidden layer dimension of H, and output dimension C. \n",
        "    We train the network with a MLE loss function. The network uses a ReLU\n",
        "    nonlinearity after each fully connected layer except for the last. \n",
        "    The outputs of the last fully-connected layer are passed through\n",
        "    a sigmoid. \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        hidden_sizes: Sequence[int],\n",
        "        output_size: int,\n",
        "        num_layers: int,\n",
        "        opt: str, \n",
        "    ):\n",
        "        \"\"\"Initialize the model. Weights are initialized to small random values\n",
        "        and biases are initialized to zero. Weights and biases are stored in\n",
        "        the variable self.params, which is a dictionary with the following\n",
        "        keys:\n",
        "        W1: 1st layer weights; has shape (D, H_1)\n",
        "        b1: 1st layer biases; has shape (H_1,)\n",
        "        ...\n",
        "        Wk: kth layer weights; has shape (H_{k-1}, C)\n",
        "        bk: kth layer biases; has shape (C,)\n",
        "        Parameters:\n",
        "            input_size: The dimension D of the input data\n",
        "            hidden_size: List [H1,..., Hk] with the number of neurons Hi in the\n",
        "                hidden layer i\n",
        "            output_size: output dimension C\n",
        "            num_layers: Number of fully connected layers in the neural network\n",
        "        \"\"\"\n",
        "        self.input_size = input_size\n",
        "        self.hidden_sizes = hidden_sizes\n",
        "        self.output_size = output_size\n",
        "        self.num_layers = num_layers\n",
        "        self.opt = opt\n",
        "        self.t = 0\n",
        "\n",
        "        self.m = defaultdict(int)\n",
        "        self.v = defaultdict(int)\n",
        "\n",
        "        assert len(hidden_sizes) == (num_layers - 1)\n",
        "        sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        self.params = {}\n",
        "        for i in range(1, self.num_layers + 1):\n",
        "            self.params[\"W\" + str(i)] = np.random.randn(\n",
        "                sizes[i - 1], sizes[i]\n",
        "            ) / np.sqrt(sizes[i - 1])\n",
        "            self.params[\"b\" + str(i)] = np.zeros(sizes[i])\n",
        "        \n",
        "            \n",
        "\n",
        "    def linear(self, W: np.ndarray, X: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Fully connected (linear) layer.\n",
        "        Parameters:\n",
        "            W: the weight matrix\n",
        "            X: the input data\n",
        "            b: the bias\n",
        "        Returns:\n",
        "            the output\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "        output = np.dot(X,W) + b\n",
        "        return output\n",
        "\n",
        "    def relu(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Rectified Linear Unit (ReLU).\n",
        "        Parameters:\n",
        "            X: the input data\n",
        "        Returns:\n",
        "            the output\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "        \n",
        "        output = np.maximum(0,X)\n",
        "        return output\n",
        "\n",
        "    def relu_grad(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Gradient of Rectified Linear Unit (ReLU).\n",
        "        Parameters:\n",
        "            X: the input data\n",
        "        Returns:\n",
        "            the output data\n",
        "        \"\"\"\n",
        "        # TODO: implement me\n",
        "     \n",
        "        \n",
        "        X = np.where(X >= 0, 1, 0)\n",
        "        return X\n",
        "        \n",
        "    def sigmoid(self, x: np.ndarray) -> np.ndarray:\n",
        "        return np.where(x<0, np.exp(x)/(1+np.exp(x)), 1/(1+np.exp(-x)))\n",
        "  \n",
        "    def sigmoid_grad(self, x: np.ndarray) -> np.ndarray:\n",
        "\t\t# compute the derivative of the sigmoid function\n",
        "      return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
        "    \n",
        "    def mse(self, y: np.ndarray, p: np.ndarray) -> np.ndarray:\n",
        "      # TODO implement this\n",
        "      return np.mean((y - p)**2)\n",
        "\n",
        "      #pass\n",
        "\n",
        "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute the outputs for all of the data samples.\n",
        "        Hint: this function is also used for prediction.\n",
        "        Parameters:\n",
        "            X: Input data of shape (N, D). Each X[i] is a training or\n",
        "                testing sample\n",
        "        Returns:\n",
        "            Matrix of shape (N, C) \n",
        "        \"\"\"\n",
        "        # TODO: implement me. You'll want to store the output of each layer in\n",
        "        # self.outputs as it will be used during back-propagation. You can use\n",
        "        # the same keys as self.params. You can use functions like\n",
        "        # self.linear, self.relu, and self.mse in here.\n",
        "\n",
        "        self.outputs = {}\n",
        "        self.outputs['A0'] = X.copy()\n",
        "        \n",
        "        for l in range(1, self.num_layers):\n",
        "          self.outputs[\"Z\" + str(l)] = self.linear(self.params[\"W\" + str(l)], self.outputs[\"A\" + str(l-1)], self.params[\"b\" + str(l)])\n",
        "          self.outputs[\"A\" + str(l)] = self.relu(self.outputs[\"Z\" + str(l)])\n",
        "          \n",
        "        # Pass output of second last layer through last layer \n",
        "        self.outputs[\"Z\" + str(self.num_layers)] = self.linear(self.params[\"W\" + str(self.num_layers)], self.outputs[\"A\" + str(self.num_layers-1)], self.params[\"b\" + str(self.num_layers)])\n",
        "        self.outputs[\"A\" + str(self.num_layers)] = self.sigmoid(self.outputs[\"Z\" + str(self.num_layers)])\n",
        "        \n",
        "        return self.outputs[\"A\" + str(self.num_layers)]\n",
        "\n",
        "    def backward(self, y: np.ndarray) -> float:\n",
        "        \"\"\"Perform back-propagation and compute the gradients and losses.\n",
        "        Parameters:\n",
        "            y: training value targets\n",
        "        Returns:\n",
        "            Total loss for this batch of training samples\n",
        "        \"\"\"\n",
        "        self.gradients = {}\n",
        "        # TODO: implement me. You'll want to store the gradient of each\n",
        "        # parameter in self.gradients as it will be used when updating each\n",
        "        # parameter and during numerical gradient checks. You can use the same\n",
        "        # keys as self.params. You can add functions like self.linear_grad,\n",
        "        # self.relu_grad, and self.softmax_grad if it helps organize your code.\n",
        "\n",
        "        num_samples = (y.shape[0]*y.shape[1])\n",
        "        #self.outputs[\"A\" + str(self.num_layers)].shape[1]\n",
        "    \n",
        "        #Compute loss \n",
        "        loss = self.mse(self.outputs[\"A\" + str(self.num_layers)], y)\n",
        "        \n",
        "        #upstream gradient of last layer \n",
        "        self.gradients['A' + str(self.num_layers)] = 2 * (self.outputs[\"A\" + str(self.num_layers)] - y) / num_samples\n",
        "        \n",
        "        # Downstream gradient of last layer \n",
        "        self.gradients['Z' + str(self.num_layers)] = self.gradients['A' + str(self.num_layers)] * self.sigmoid_grad(self.outputs[\"Z\" + str(self.num_layers)])        \n",
        "        self.gradients[\"W\" + str(self.num_layers)] = np.dot(self.outputs[\"A\" + str(self.num_layers - 1)].T, self.gradients['Z' + str(self.num_layers)])   \n",
        "        self.gradients[\"b\" + str(self.num_layers)] = np.sum(self.gradients['Z'+ str(self.num_layers)], axis=0)\n",
        "        \n",
        "        dAPrev = np.dot(self.gradients['Z' + str(self.num_layers)], self.params[\"W\" + str(self.num_layers)].T)\n",
        "       \n",
        "        \n",
        "        for l in range(self.num_layers - 1, 0, -1):\n",
        "          self.gradients[\"Z\" + str(l)] = dAPrev * self.relu_grad(self.outputs[\"Z\" + str(l)])\n",
        "          self.gradients[\"W\" + str(l)] = np.dot(self.outputs[\"A\" + str(l - 1)].T,self.gradients[\"Z\" + str(l)])\n",
        "          self.gradients[\"b\" + str(l)] = np.sum(self.gradients[\"Z\" + str(l)], axis = 0)\n",
        "          if l > 1:\n",
        "            dAPrev = self.gradients[\"Z\" + str(l)].dot(self.params[\"W\" + str(l)].T)\n",
        "        return loss \n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        lr: float = 0.001,\n",
        "        b1: float = 0.9,\n",
        "        b2: float = 0.999,\n",
        "        eps: float = 1e-8\n",
        "    ):\n",
        "        \"\"\"Update the parameters of the model using the previously calculated\n",
        "        gradients.\n",
        "        Parameters:\n",
        "            lr: Learning rate\n",
        "            b1: beta 1 parameter (for Adam)\n",
        "            b2: beta 2 parameter (for Adam)\n",
        "            eps: epsilon to prevent division by zero (for Adam)\n",
        "            opt: optimizer, either 'SGD' or 'Adam'\n",
        "        \"\"\"\n",
        "        # TODO: implement me. You'll want to add an if-statement that can\n",
        "        # handle updates for both SGD and Adam depending on the value of opt.\n",
        "        if self.opt == \"SGD\":\n",
        "          for l in range(1, self.num_layers + 1):\n",
        "              self.params['W'+str(l)] += -lr * self.gradients['W'+str(l)] \n",
        "              self.params['b'+str(l)] += -lr * self.gradients['b'+str(l)] \n",
        "              \n",
        "        elif self.opt == \"adam\":\n",
        "          \n",
        "          ## dw, db are from current minibatch\n",
        "          ## momentum beta 1\n",
        "\n",
        "          self.t += 1 \n",
        "          for l in range(1, self.num_layers+1 ):\n",
        "            self.m['W'+str(l)] = b1*self.m['W'+str(l)] + (1-b1)*self.gradients['W'+str(l)] \n",
        "            self.m['b'+str(l)] = b1*self.m['b'+str(l)] + (1-b1)*self.gradients['b'+str(l)] \n",
        "            self.v['W'+str(l)] = b2*self.v['W'+str(l)] + (1-b2)*(self.gradients['W'+str(l)]**2)\n",
        "            self.v['b'+str(l)] = b2*self.v['b'+str(l)] + (1-b2)*(self.gradients['b'+str(l)]**2)\n",
        "\n",
        "            m_dw_corr = self.m['W'+str(l)]/(1-b1**self.t)\n",
        "            m_db_corr = self.m['b'+str(l)]/(1-b1**self.t)\n",
        "            v_dw_corr = self.v['W'+str(l)]/(1-b2**self.t)\n",
        "            v_db_corr = self.v['b'+str(l)]/(1-b2**self.t)\n",
        "\n",
        "          ## update weights and biases\n",
        "            self.params['W'+str(l)] -=  lr*(m_dw_corr/(np.sqrt(v_dw_corr)+eps))\n",
        "            self.params['b'+str(l)] -=  lr*(m_db_corr/(np.sqrt(v_db_corr)+eps)) \n",
        "        \n",
        "  \n",
        "        #pass"
      ],
      "metadata": {
        "id": "NKvvlCaz8XPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kq4y5whKx5SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5X9DO-5_Bzn"
      },
      "source": [
        "The cell below initializes a toy dataset and corresponding model which will allow you to check your forward and backward pass by using a numeric gradient check. Note that we set a random seed for repeatable experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "358jAXcc_Bzn"
      },
      "outputs": [],
      "source": [
        "input_size = 2\n",
        "hidden_size = 10\n",
        "num_classes = 3\n",
        "num_inputs = 5\n",
        "optimizer = 'adam'\n",
        "\n",
        "\n",
        "def init_toy_model(num_layers):\n",
        "    \"\"\"Initializes a toy model\"\"\"\n",
        "    np.random.seed(0)\n",
        "    hidden_sizes = [hidden_size] * (num_layers - 1)\n",
        "    return NeuralNetwork(input_size, hidden_sizes, num_classes, num_layers, optimizer)\n",
        "\n",
        "def init_toy_data():\n",
        "    \"\"\"Initializes a toy dataset\"\"\"\n",
        "    np.random.seed(10)\n",
        "    X = np.random.randn(num_inputs, input_size)\n",
        "    y = np.random.randn(num_inputs, num_classes)\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh_v9biP_Bzn"
      },
      "source": [
        "# Implement forward and backward pass\n",
        "\n",
        "The first thing you will do is implement the forward pass of your neural network. The forward pass should be implemented in the `forward` function. You can use helper functions like `linear`, `relu`, and `sigmoid` to help organize your code.\n",
        "\n",
        "Next, you will implement the backward pass using the backpropagation algorithm. Backpropagation will compute the gradient of the loss with respect to the model parameters `W1`, `b1`, ... etc. Use an MSE for loss calcuation. Fill in the code blocks in `NeuralNetwork.backward`. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjAwpT2z_Bzo"
      },
      "source": [
        "# Gradient  check\n",
        "\n",
        "If you have implemented your forward pass through the network correctly, you can use the following cell to debug your backward pass with a numeric gradient check. If your backward pass has been implemented correctly, the max relative error between your analytic solution and the numeric solution should be around 1e-7 or less for all parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UZM47qUP_Bzo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f1c47d-7ea9-4d22-cd29-daaec29c7696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.6138157869359697\n",
            "W1 max relative error: 2.637235e-08\n",
            "b1 max relative error: 8.262601e-10\n",
            "W2 max relative error: 7.608357e-09\n",
            "b2 max relative error: 1.590805e-10\n",
            "1.640888163195227\n",
            "W1 max relative error: 3.064017e-07\n",
            "b1 max relative error: 4.341296e-09\n",
            "W2 max relative error: 3.652332e-08\n",
            "b2 max relative error: 9.002422e-10\n",
            "W3 max relative error: 1.597785e-08\n",
            "b3 max relative error: 1.919867e-10\n"
          ]
        }
      ],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "from utils.gradient_check import eval_numerical_gradient\n",
        "\n",
        "X, y = init_toy_data()\n",
        "\n",
        "def f(W):\n",
        "    net.forward(X)\n",
        "    op=net.backward(y)\n",
        "    \n",
        "    return op\n",
        "\n",
        "for num in [2, 3]:\n",
        "    net = init_toy_model(num)\n",
        "    net.forward(X)\n",
        "    net.backward(y)\n",
        "    gradients = deepcopy(net.gradients)\n",
        "\n",
        "    for param_name in net.params:\n",
        "        param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
        "        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, gradients[param_name])))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}